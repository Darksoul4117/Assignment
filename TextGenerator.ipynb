{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing\nimport numpy\nimport sys\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\n\nimport nltk\nnltk.download('stopwords')\n\n#load data\nfile = open(\"Frankenstein.txt\").read()\n\n#tokenization\n#standardization\ndef tokenize_words(input):\n  input = input.lower()\n  tokenizer = RegexpTokenizer(r'\\w+')\n  tokens = tokenizer.tokenize(input)\n  filtered = filter(lambda token: token not in stopwords.words('english'),tokens)\n  return \" \".join(filtered)\nprocessed_inputs = tokenize_words(file)\n\n#chars to numbers\nchars = sorted(list(set(processed_inputs)))\nchar_to_num = dict((c,i) for i, c in enumerate(chars))\n\n#check if words to chars to num(?!) has worked?\ninput_len = len(processed_inputs)\nvocab_len = len(chars)\nprint(\"Total number of characters:\",input_len)\nprint(\"Total vocab:\",vocab_len)\n\n#seq_length\nseq_length = 100\nx_data = []\ny_data = []\n\n#loop through the sequence\nfor i in range(0,input_len-seq_length, 1):\n  in_seq = processed_inputs[i:i + seq_length]\n  out_seq = processed_inputs[i + seq_length]\n  x_data.append([char_to_num[char] for char in in_seq])\n  y_data.append(char_to_num[out_seq])\nn_patterns = len(x_data)\nprint(\"Total Patterns:\",n_patterns)\n\n#convert input sequence to np array and so on\nX = numpy.reshape(x_data, (n_patterns, seq_length, 1))\nX = X/float(vocab_len)\n\n#one-hot encoding\ny = np_utils.to_categorical(y_data)\n\n#creating the model\nmodel = Sequential()\nmodel.add(LSTM(256, input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(256, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y.shape[1], activation='softmax'))\n\n#compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n#saving weights\nfilepath = 'model_weights_saved.hdf5'\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose = 1,save_best_only=True, mode='min')\ndesired_callbacks = [checkpoint]\n\n#fit model and let it train\nmodel.fit(X,y, epochs=4, batch_size=256, callbacks=desired_callbacks)\n\n#recompile model with the saved weights\nfilename = \"model_weights_saved.hdf5\"\nmodel.load_weights(filename)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# ouput of the model back into characters\nnum_to_chars = dict((i,c) for i,c in enumerate(chars))\n\n# random seed to help generate\nstart = numpy.random.randit(0, len(x_data)-1)\npattern = x_data[start]\nprint('Random Seed:')\nprint(\"\\\"\",''.join([num_to_chars[value] for value in pattern]),\"\\\"\")\n\n# generate the text\nfor i in range(1000):\n  x = numpy.reshape(pattern, (1,len(pattern), 1))\n  x = x/float(vocab_len)\n  prediction = model.predict(x, verbose=0)\n  index = numpy.argmax(prediction)\n  result = num_to_chars[index]\n  seq_in = [num_to_chars[value] for value in pattern]\n  sys.stdout.write(result)\n  pattern.append(index)\n  pattern = pattern[1:len(pattern)]","metadata":{},"execution_count":null,"outputs":[]}]}